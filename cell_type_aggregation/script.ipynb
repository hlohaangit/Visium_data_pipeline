{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub group cell type csv\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "def create_cell_type_dictionary(csv_file_path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    # Create dictionary to store results\n",
    "    cell_type_dict = {}\n",
    "    \n",
    "    # Process each row\n",
    "    for index, row in df.iterrows():\n",
    "        main_type = row['Main_Types']\n",
    "        sub_types = row['Sub_Types']\n",
    "        \n",
    "        # Split subtypes by '.' to get individual subtypes\n",
    "        subtype_list = sub_types.split('.')\n",
    "        \n",
    "        # Add to dictionary\n",
    "        cell_type_dict[main_type] = subtype_list\n",
    "    \n",
    "    return cell_type_dict\n",
    "\n",
    "# Create the dictionary\n",
    "csv_data = '/orange/pinaki.sarder/j.fermin/SpatNet/Data/Counts/Cell_SubTypes_Grouped.csv'\n",
    "\n",
    "result = create_cell_type_dictionary(csv_data)\n",
    "\n",
    "# Print the results\n",
    "print(\"Cell Type Dictionary:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(result)\n",
    "\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Visium_processor.py\n",
    "\n",
    "\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "import openslide\n",
    "from openslide import OpenSlide\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "import h5py\n",
    "import zarr\n",
    "import time\n",
    "from functools import partial\n",
    "import gc\n",
    "\n",
    "class OptimizedWSIProcessor:\n",
    "    def __init__(self, wsi_path, patch_size=224, target_mpp=0.5):\n",
    "        self.wsi_path = wsi_path\n",
    "        self.patch_size = patch_size\n",
    "        self.target_mpp = target_mpp\n",
    "        self.slide = None\n",
    "        self.scale_factor = 1.0\n",
    "        self.current_mpp = 0.25\n",
    "        self.is_tiff = wsi_path.endswith('.tif')\n",
    "        self.tiff_image = None\n",
    "        \n",
    "        self._initialize_slide()\n",
    "    \n",
    "    def _initialize_slide(self):\n",
    "        \"\"\"Initialize slide and calculate scale factor once\"\"\"\n",
    "        try:\n",
    "            if self.is_tiff:\n",
    "                # For TIFF files, load once and keep in memory if reasonable size\n",
    "                self.tiff_image = cv2.imread(self.wsi_path)\n",
    "                if self.tiff_image is None:\n",
    "                    raise ValueError(f\"Could not read TIFF file: {self.wsi_path}\")\n",
    "                print(f\"Loaded TIFF image: {self.tiff_image.shape}\")\n",
    "            else:\n",
    "                # For SVS files, use OpenSlide\n",
    "                self.slide = OpenSlide(self.wsi_path)\n",
    "                self.scale_factor, self.current_mpp = self._calculate_scale_factor()\n",
    "                print(f\"SVS slide initialized. Scale factor: {self.scale_factor:.3f}, Current MPP: {self.current_mpp:.3f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing slide: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _calculate_scale_factor(self):\n",
    "        \"\"\"Calculate scale factor for target MPP\"\"\"\n",
    "        try:\n",
    "            if 'openslide.mpp-x' in self.slide.properties:\n",
    "                current_mpp = float(self.slide.properties['openslide.mpp-x'])\n",
    "                print(current_mpp, 'harshit')\n",
    "            elif 'aperio.MPP' in self.slide.properties:\n",
    "                print(current_mpp, 'harshit')\n",
    "\n",
    "                current_mpp = float(self.slide.properties['aperio.MPP'])\n",
    "            else:\n",
    "                current_mpp = 0.25\n",
    "                print(f\"Warning: MPP not found. Using default: {current_mpp}\")\n",
    "            \n",
    "            scale_factor = current_mpp / self.target_mpp\n",
    "            return scale_factor, current_mpp\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating scale factor: {e}\")\n",
    "            return 1.0, 0.25\n",
    "    \n",
    "    def extract_batch_patches(self, coordinates_batch):\n",
    "        \"\"\"Extract multiple patches in one operation\"\"\"\n",
    "        patches = []\n",
    "        \n",
    "        for x, y in coordinates_batch:\n",
    "            try:\n",
    "                if self.is_tiff:\n",
    "                    patch = self._extract_tiff_patch(x, y)\n",
    "                else:\n",
    "                    patch = self._extract_svs_patch(x, y)\n",
    "                patches.append(patch)\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting patch at ({x}, {y}): {e}\")\n",
    "                patches.append(np.zeros((self.patch_size, self.patch_size, 3), dtype=np.uint8))\n",
    "        \n",
    "        return patches\n",
    "    \n",
    "    def _extract_tiff_patch(self, x, y):\n",
    "        \"\"\"Extract patch from TIFF image\"\"\"\n",
    "        half_patch = self.patch_size // 2\n",
    "        x_start = max(0, int(x - half_patch))\n",
    "        y_start = max(0, int(y - half_patch))\n",
    "        x_end = min(self.tiff_image.shape[1], int(x + half_patch))\n",
    "        y_end = min(self.tiff_image.shape[0], int(y + half_patch))\n",
    "        \n",
    "        patch = self.tiff_image[y_start:y_end, x_start:x_end]\n",
    "        \n",
    "        if patch.shape[:2] != (self.patch_size, self.patch_size):\n",
    "            patch = cv2.resize(patch, (self.patch_size, self.patch_size))\n",
    "        \n",
    "        return patch\n",
    "    \n",
    "    def _extract_svs_patch(self, x, y):\n",
    "        \"\"\"Extract patch from SVS file\"\"\"\n",
    "        original_patch_size = int(self.patch_size * self.scale_factor)\n",
    "        half_patch = original_patch_size // 2\n",
    "        \n",
    "        patch_pil = self.slide.read_region(\n",
    "            (int(x - half_patch), int(y - half_patch)), \n",
    "            0, \n",
    "            (original_patch_size, original_patch_size)\n",
    "        )\n",
    "        \n",
    "        patch_rgb = patch_pil.convert('RGB')\n",
    "        patch_resized = patch_rgb.resize((self.patch_size, self.patch_size), Image.LANCZOS)\n",
    "        return np.array(patch_resized)\n",
    "    \n",
    "    def __del__(self):\n",
    "        \"\"\"Clean up resources\"\"\"\n",
    "        if self.slide:\n",
    "            self.slide.close()\n",
    "\n",
    "def process_batch_worker(batch_data):\n",
    "    \"\"\"Worker function for batch processing\"\"\"\n",
    "    wsi_path, coordinates_batch, spot_ids_batch, cell_type_vectors_batch, output_dirs, patch_size, target_mpp = batch_data\n",
    "    \n",
    "    # Initialize processor for this worker\n",
    "    processor = OptimizedWSIProcessor(wsi_path, patch_size, target_mpp)\n",
    "    \n",
    "    # Extract patches for this batch\n",
    "    patches = processor.extract_batch_patches(coordinates_batch)\n",
    "    \n",
    "    # Save results\n",
    "    results = []\n",
    "    for i, (patch, spot_id, cell_type_vector) in enumerate(zip(patches, spot_ids_batch, cell_type_vectors_batch)):\n",
    "        try:\n",
    "            # Save patch as PNG\n",
    "            patch_path = output_dirs['patches'] / f\"{spot_id}.png\"\n",
    "            Image.fromarray(patch).save(patch_path)\n",
    "            \n",
    "            # Save patch as numpy array\n",
    "            # array_path = output_dirs['arrays'] / f\"{spot_id}.npy\"\n",
    "            # np.save(array_path, patch)\n",
    "            \n",
    "            # Save cell type vector\n",
    "            cell_type_tensor = torch.tensor(cell_type_vector, dtype=torch.float32)\n",
    "            cell_type_path = output_dirs['cell_types'] / f\"{spot_id}.pt\"\n",
    "            torch.save(cell_type_tensor, cell_type_path)\n",
    "            \n",
    "            results.append(True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving spot {spot_id}: {e}\")\n",
    "            results.append(False)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def save_batch_hdf5(patches_batch, spot_ids_batch, cell_types_batch, hdf5_path):\n",
    "    \"\"\"Save batch data to HDF5 for faster I/O\"\"\"\n",
    "    with h5py.File(hdf5_path, 'a') as f:\n",
    "        for patch, spot_id, cell_type in zip(patches_batch, spot_ids_batch, cell_types_batch):\n",
    "            grp = f.create_group(spot_id)\n",
    "            grp.create_dataset('patch', data=patch, compression='gzip')\n",
    "            grp.create_dataset('cell_type', data=cell_type, compression='gzip')\n",
    "\n",
    "def process_visium_data_optimized(h5ad_file, wsi_file, csv_file, output_dir, \n",
    "                                patch_size=224, target_mpp=0.5, batch_size=100, \n",
    "                                n_workers=None, save_hdf5=True):\n",
    "    \"\"\"\n",
    "    Optimized version with batch processing and parallelization\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Set default number of workers\n",
    "    if n_workers is None:\n",
    "        n_workers = min(mp.cpu_count() - 1, 8)  # Leave one core free, max 8 workers\n",
    "    \n",
    "    print(f\"Using {n_workers} workers with batch size {batch_size}\")\n",
    "    \n",
    "    # Create output directories\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dirs = {\n",
    "        'patches': output_dir / \"patches\",\n",
    "        'arrays': output_dir / \"arrays\", \n",
    "        'cell_types': output_dir / \"cell_sub_types\"\n",
    "    }\n",
    "    \n",
    "    for dir_path in output_dirs.values():\n",
    "        dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Read data\n",
    "    print(\"Loading data...\")\n",
    "    adata = sc.read_h5ad(h5ad_file)\n",
    "    spatial_coords = adata.obsm['spatial']\n",
    "    spot_ids = adata.obs.index.tolist()\n",
    "    \n",
    "    # Read CSV with optimized pandas settings\n",
    "    df_cell_types = pd.read_csv(csv_file, index_col=0)\n",
    "    print(f\"Loaded {len(spot_ids)} spots and {df_cell_types.shape[0]} cell types\")\n",
    "    \n",
    "    # Prepare cell type vectors for all spots\n",
    "    cell_type_vectors = []\n",
    "    for spot_id in spot_ids:\n",
    "        if spot_id in df_cell_types.columns:\n",
    "            cell_type_vectors.append(df_cell_types[spot_id].values)\n",
    "        else:\n",
    "            cell_type_vectors.append(np.zeros(16))\n",
    "    \n",
    "    # Create batches\n",
    "    n_spots = len(spot_ids)\n",
    "    n_batches = (n_spots + batch_size - 1) // batch_size\n",
    "    \n",
    "    print(f\"Processing {n_spots} spots in {n_batches} batches...\")\n",
    "    \n",
    "    # Prepare batch data\n",
    "    batch_data_list = []\n",
    "    for i in range(0, n_spots, batch_size):\n",
    "        end_idx = min(i + batch_size, n_spots)\n",
    "        batch_data = (\n",
    "            wsi_file,\n",
    "            spatial_coords[i:end_idx],\n",
    "            spot_ids[i:end_idx],\n",
    "            cell_type_vectors[i:end_idx],\n",
    "            output_dirs,\n",
    "            patch_size,\n",
    "            target_mpp\n",
    "        )\n",
    "        batch_data_list.append(batch_data)\n",
    "    \n",
    "    # Process batches in parallel\n",
    "    successful_extractions = 0\n",
    "    \n",
    "    if n_workers > 1:\n",
    "        # Use ProcessPoolExecutor for CPU-bound tasks\n",
    "        with ProcessPoolExecutor(max_workers=n_workers) as executor:\n",
    "            futures = [executor.submit(process_batch_worker, batch_data) for batch_data in batch_data_list]\n",
    "            \n",
    "            for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing batches\"):\n",
    "                try:\n",
    "                    results = future.result()\n",
    "                    successful_extractions += sum(results)\n",
    "                except Exception as e:\n",
    "                    print(f\"Batch processing error: {e}\")\n",
    "    else:\n",
    "        # Sequential processing for debugging\n",
    "        for batch_data in tqdm(batch_data_list, desc=\"Processing batches\"):\n",
    "            try:\n",
    "                results = process_batch_worker(batch_data)\n",
    "                successful_extractions += sum(results)\n",
    "            except Exception as e:\n",
    "                print(f\"Batch processing error: {e}\")\n",
    "    \n",
    "    # Optional: Save consolidated HDF5 file\n",
    "    if save_hdf5:\n",
    "        print(\"Saving consolidated HDF5 file...\")\n",
    "        hdf5_path = output_dir / \"consolidated_data.h5\"\n",
    "        # This would require a separate implementation for large-scale HDF5 writing\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    # Save summary\n",
    "    summary = {\n",
    "        'h5ad_file': h5ad_file,\n",
    "        'wsi_file': wsi_file,\n",
    "        'csv_file': csv_file,\n",
    "        'total_spots': n_spots,\n",
    "        'successful_extractions': successful_extractions,\n",
    "        'patch_size': patch_size,\n",
    "        'target_mpp': target_mpp,\n",
    "        'batch_size': batch_size,\n",
    "        'n_workers': n_workers,\n",
    "        'processing_time_seconds': processing_time,\n",
    "        'spots_per_second': n_spots / processing_time if processing_time > 0 else 0\n",
    "    }\n",
    "    \n",
    "    summary_path = output_dir / \"summary.txt\"\n",
    "    with open(summary_path, 'w') as f:\n",
    "        for key, value in summary.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "    \n",
    "    print(f\"Processing complete! Processed {successful_extractions}/{n_spots} spots in {processing_time:.2f}s\")\n",
    "    print(f\"Speed: {n_spots/processing_time:.2f} spots/second\")\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "base = '/blue/pinaki.sarder/j.fermin/Annotations/Data/XY04_IU-21-020F'\n",
    "filename = base.split('/')[-1]\n",
    "h5ad_file = '/blue/pinaki.sarder/j.fermin/CellAtlas/data/Kidney/H5AD_files/'+filename+'.h5ad'\n",
    "wsi_file = base+'/'+filename+'.svs'\n",
    "\n",
    "cvs_file = '/orange/pinaki.sarder/j.fermin/SpatNet/Data/Counts/FFPE/CellTypes_SpotLevel/All_Counts/'+filename+'counts.csv'\n",
    "output = '/orange/pinaki.sarder/h.lohaan/Hari_data_pipeline/output_sub_types/'+filename+''\n",
    "# Validate input files\n",
    "for file_path in [h5ad_file, wsi_file, cvs_file]:\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found: {file_path}\")\n",
    "        \n",
    "# Process the data\n",
    "summary = process_visium_data_optimized(\n",
    "    h5ad_file, \n",
    "    wsi_file, \n",
    "    cvs_file, \n",
    "    output\n",
    ")\n",
    "\n",
    "print(\"Processing complete!\")\n",
    "print(f\"Output saved to: {output}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
